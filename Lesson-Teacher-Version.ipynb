{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, Type I and Type II error rates\n",
    "\n",
    "When we perform a hypothesis test, we may reject or fail to reject the null hypothesis about a population parameter. In a hypothesis test, the null hypothesis states that there is no difference between our groups, whereas the alternative hypothesis states that there is a difference between the groups we're testing. We reject a null hypothesis if the p-value we obtain is less than $\\alpha$, the significance level of our test. $\\alpha$ is the probability of rejecting a null hypothesis when it's actually true\n",
    "\n",
    "There are four possible outcomes when performing statistical hypothesis tests: \n",
    "\n",
    "<img src=\"images/confusion_matrix.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Imagine you are running a clinical trial for a new drug, and you want to test whether patient symptoms improve more rapidly after treatment with the drug than after a placebo treatment.\n",
    "\n",
    "**What's the null hypothesis in this case?**\n",
    "\n",
    "**What are the Type I and Type II errors in this context?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Null hypothesis: Patient symptoms after treatment with the drug are indistinguishable from a placebo treatment. \n",
    "\n",
    "> Type I error would indicate the drug is more effective than the placebo in treating patient symptoms when in fact it is not.\n",
    "\n",
    "> Type II error would indicate that the drug is not more effective than placebo when in fact the drug is more effective than the placebo in treating patient symptoms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Type I Error \n",
    "If you reject a null hypothesis that is actually correct, you are making a type I error.\n",
    "\n",
    "* $\\alpha$, the significance level of a hypothesis test, is the probability of making a type I error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Type I Error simulations \n",
    "\n",
    "Imagine we have two samples of scores from the same population of scores. This population is normally distributed with mean 10 and standard deviation 1.\n",
    "\n",
    "Although we know that the two samples are from the same population of scores, there is still a small probability of seeing a large difference between mean sample values and of incorrectly rejecting the null hypothesis that the samples come from the same population, that is, of making a Type I error. \n",
    "\n",
    "**Let's run 1000 two-sample t-tests to compute type 1 error rate.** \n",
    "\n",
    "We will repeatedly (`n_simulations = 1000` times):\n",
    "* take two independent samples from the same population, \n",
    "* compute a two-sided t-test with significance level $\\alpha = 0.05$, and \n",
    "* keep count of the number of times we reject the null hypothesis, even though we know it to be true \n",
    "\n",
    "The goal is to compute type I error rate.\n",
    "\n",
    "Remember, type I error is when you reject the null hypothesis given it is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we run our simulations, **what are the null and alternative hypotheses for the tests we're performing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> The null hypothesis is that the two samples come from the same population. The alternative hypothesis is that they do not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# relevant imports\n",
    "import scipy.stats as stats\n",
    "import numpy as np \n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create an instance of a normal continuous random variable with mean 10 and standard deviation 1\n",
    "scores_population = stats.norm(loc=10, scale=1)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = scores_population.rvs(n_sample_size), scores_population.rvs(n_sample_size)\n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    if result[1] < alpha:\n",
    "        c+=1\n",
    "\n",
    "type_1_error_rate = c/n_simulations\n",
    "\n",
    "print(\"Out of {} tests performed, the null hypothesis was incorrectly rejected {} times.\".format(n_simulations, c))\n",
    "print(\"\\nThe type I error rate is {}.\".format(type_1_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Type II error \n",
    "\n",
    "If we fail to reject a null hypothesis that is actually false, you are making a type II error. \n",
    "* We use $\\beta$ to denote the probability of making a type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Type II Error simulations \n",
    "\n",
    "Now, imagine that we have two samples of scores from two different populations that are normally distributed, one with mean 5 and standard deviation 1, and the other with mean 6 and standard deviation 2. \n",
    "\n",
    "Even though we know that the two samples are from different populations, there is still the chance that we will think the samples come from the same population even though we know they do not, that is, there is a chance we will fail to reject the null hypothesis. In this case, we would be committing a Type II error. \n",
    "\n",
    "**Let's run some simulations to compute type 2 error rate in this scenario.** \n",
    "\n",
    "You will repeatedly (`n_simulations = 1000`):\n",
    "* take two samples, with sample size equal to 25, from the two different populations, \n",
    "* compute a two-sided t-test with significance level $\\alpha = 0.05$, and \n",
    "* keep count of the number of times we fail to reject the null hypothesis, even though we know it to be false. \n",
    "\n",
    "The goal is to compute type II error rate. Remember, type II error is when you fail to reject the null hypothesis given it is false.\n",
    "\n",
    "**What are the null and alternative hypotheses in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> In this example, the null hypothesis is that the two samples come from the same population. The alternative hypothesis is that they do not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instances of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = None\n",
    "scores_population_2 = None\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw from each population to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis if the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you fail to reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    \n",
    "    sample1, sample2 = None, None\n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of whether the null hypothesis was rejected or not \n",
    "    None\n",
    "\n",
    "type_2_error_rate = None\n",
    "\n",
    "print(\"Out of {} tests performed, the null hypothesis was not rejected {} times.\".format(n_simulations, c))\n",
    "print(\"\\nThe type II error rate is {}.\".format(type_2_error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instances of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = stats.norm(loc=5, scale=1)\n",
    "scores_population_2 = stats.norm(loc=6, scale=2)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw from each population to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis if the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you fail to reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    \n",
    "    sample1, sample2 = scores_population_1.rvs(n_sample_size), scores_population_2.rvs(n_sample_size) \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of whether the null hypothesis was rejected or not\n",
    "    if result[1] > 0.05:\n",
    "        c+=1\n",
    "\n",
    "type_2_error_rate = c/n_simulations\n",
    "\n",
    "print(\"Out of {} tests performed, the null hypothesis was not rejected {} times.\".format(n_simulations, c))\n",
    "print(\"\\nThe type II error rate is {}.\".format(type_2_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens in you change the sample size to 10 instead of 25?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> You should obtain a larger type II error rate. Run code again with `n_sample_size = 10`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instances of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = stats.norm(loc=5, scale=1)\n",
    "scores_population_2 = stats.norm(loc=6, scale=2)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 10 \n",
    "\n",
    "# You reject the null hypothesis if the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you fail to reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    \n",
    "    sample1, sample2 = scores_population_1.rvs(n_sample_size), scores_population_2.rvs(n_sample_size) \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of whether the null hypothesis was rejected or not\n",
    "    if result[1] > 0.05:\n",
    "        c+=1\n",
    "\n",
    "type_2_error_rate = c/n_simulations\n",
    "\n",
    "print(\"Out of {} tests performed, the null hypothesis was not rejected {} times.\".format(n_simulations, c))\n",
    "print(\"\\nThe type II error rate is {}.\".format(type_2_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we perform a statistical hypothesis test, we want to make sure that if the null hypothesis isn't true, we're able to reject the null hypothesis. We want to be able to detect a difference between the groups if there is one.\n",
    "\n",
    "Statistical power is the probability that we will reject the null hypothesis given it is actually false. Thus, $\\text{power} = 1 - \\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The statistical power of a hypothesis test is a function of:\n",
    "* the sample size, \n",
    "* the significance level $\\alpha$, and \n",
    "* the effect size or difference between the groups we are testing\n",
    "\n",
    "Typically accepted values for the power of a statistical test are greater than or equal to 0.80 or 80%. Studies with power less than 80% are said to be underpowered and require a reevaluation of experimental design or acquiring more samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Effect size \n",
    "\n",
    "When we design an experiment, we want to make sure to gather enough data to be able to detect differences between our groups, should the difference exist. The effect size is a measure of the difference between the two groups we're testing. \n",
    "\n",
    "Cohen's d, denoted by $d$, is a _standardized_ effect size measure equal to the magnitude of the difference in sample means divided by the pooled sample standard deviation of the two samples. \n",
    "* We use standardized effect sizes so we can remove the units of the variables in the effect size.  \n",
    "\n",
    "When testing the difference in the sample means of two samples, we use Cohen's d to measure the effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cohen's d\n",
    "\n",
    "Cohen's d is given by: \n",
    "\n",
    "$$ \\large d = \\frac{|\\mu_2 - \\mu_1|}{s_p},  $$\n",
    "\n",
    "where $\\mu_1$ and $\\mu_2$ are the sample means for sample 1 and 2, respectively, and $s_p$ is the pooled standard deviation of the two samples. \n",
    "\n",
    "The pooled standard deviation $s_p$ of the two samples is given by: \n",
    "\n",
    "$$ \\large s_p = \\sqrt{\\frac{\\left(n_1 -1\\right)s_1^2 + \\left(n_2 -1\\right)s_2^2 }{n_1 + n_2 - 2}}, $$\n",
    "\n",
    "where $n_1$ and $n_2$ are the sample sizes for sample 1 and sample 2, respectively, and $s_1^2$ and $s_2^2$ are the sample variances for sample 1 and sample 2, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cohen_d(sample1, sample2):\n",
    "    n1, n2 = len(sample1), len(sample2)\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    pooled_var = ((n1-1)*var1 + (n2-1)*var2)/(n1+n2-2)\n",
    "    s = np.sqrt(pooled_var)\n",
    "    \n",
    "    mean1, mean2 = np.mean(sample1), np.mean(sample2)\n",
    "    \n",
    "    return np.abs(mean2-mean1)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Effect sizes are considered to be small, medium, or large depending on the following rule of thumb: \n",
    "\n",
    "||Cohen's d|\n",
    "|--|--|\n",
    "|small|0.2|\n",
    "|medium|0.5|\n",
    "|large|0.8|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compute the effect size for the following two samples, `sample1` and `sample2`, using the function for Cohen'd written above. Is this a small, medium, or large effect size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) #for reproducibility\n",
    "rv1 = stats.norm(loc=10, scale=1)\n",
    "rv2 = stats.norm(loc=12, scale=2)\n",
    "\n",
    "sample1 = rv1.rvs(25)\n",
    "sample2 = rv2.rvs(25)\n",
    "\n",
    "cohen_d(sample1, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Large effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What if the sample size was 50?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) #for reproducibility\n",
    "\n",
    "sample1 = rv1.rvs(50)\n",
    "sample2 = rv2.rvs(50)\n",
    "\n",
    "cohen_d(sample1, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Why did the effect size increase when we increased the sample size of our samples?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> The pooled standard deviation of the samples decreases with increasing sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- - - \n",
    "\n",
    "Now that we have seen effect size in more detail, let's go back to our example to compute type II error rate using a simulation and let's compute power instead.\n",
    "\n",
    "In this example, remember that the null hypothesis is that the two samples of scores came from the same population of scores, and the alternative hypothesis is that they do not come from the same population of scores. \n",
    "\n",
    "**When you compute the power of a statistical test, what probability are you calculating?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> You compute the probability that you'll reject the null hypothesis given it's false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run 1000 simulations and compute the power of our test. Remember that $\\text{power} = 1 - \\beta$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instance of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = None\n",
    "scores_population_2 = None\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = None, None \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of the number of times you reject the null hypothesis \n",
    "    pass \n",
    "\n",
    "type_2_error_rate = None\n",
    "\n",
    "power = None \n",
    "\n",
    "print(\"Power: {}\".format(power))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instance of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = stats.norm(loc=5, scale=1)\n",
    "scores_population_2 = stats.norm(loc=6, scale=2)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = scores_population_1.rvs(n_sample_size), scores_population_2.rvs(n_sample_size) \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of the number of times you reject the null hypothesis \n",
    "    if result[1] > alpha:\n",
    "        c+=1\n",
    "\n",
    "type_2_error_rate = c/n_simulations\n",
    "\n",
    "power = 1 - type_2_error_rate\n",
    "\n",
    "print(\"Power: {}\".format(power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**If we were limited to a sample size of 10, what would be the power of our test?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instance of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = stats.norm(loc=5, scale=1)\n",
    "scores_population_2 = stats.norm(loc=6, scale=2)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 10 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = scores_population_1.rvs(n_sample_size), scores_population_2.rvs(n_sample_size) \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of the number of times you reject the null hypothesis \n",
    "    if result[1] > alpha:\n",
    "        c+=1\n",
    "\n",
    "type_2_error_rate = c/n_simulations\n",
    "\n",
    "power = 1 - type_2_error_rate\n",
    "\n",
    "print(\"Power: {}\".format(power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happened to the power of the test as we changed the sample size?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> As you can see, as the sample size of our samples decreases, so does the power of the statistical test we perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Power and effect size, sample size, and $\\alpha$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following four quantities are interrelated:\n",
    "* power\n",
    "* effect size\n",
    "* sample size\n",
    "* significance level, $\\alpha$ \n",
    "\n",
    "Given any of these three quantities, we can determine the fourth. \n",
    "\n",
    "Let's explore how power depends on effect size, sample size, and significance level $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function below allows you to compute power for any `effect_size`, `sample_size` and `alpha` combination using `n_simulations` simulated tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_power(effect_size, sample_size, alpha, n_simulations=1000):\n",
    "    \n",
    "    rv1 = stats.norm(loc=0, scale=1)\n",
    "    rv2 = stats.norm(loc=effect_size, scale=1)\n",
    "    \n",
    "    # keep a count of the times you failed to reject the null hypothesis\n",
    "    c = 0\n",
    "    for i in range(n_simulations):\n",
    "        sample1, sample2 = rv1.rvs(sample_size), rv2.rvs(sample_size)\n",
    "        result = stats.ttest_ind(sample1, sample2)\n",
    "        if result[1] > alpha:\n",
    "            c+=1\n",
    "            \n",
    "    beta = c/n_simulations\n",
    "    power = 1 - beta\n",
    "    \n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens to the power of a two-sided t-test as the sample size is changed?** \n",
    "\n",
    "**Create a plot to show how power changes as sample size changes.** \n",
    "\n",
    "Use `sample_sizes = [10, 20, 50, 100]`. \n",
    "\n",
    "Assume $\\alpha=0.05$ and that you're want to measure an effect size equal to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "sample_sizes = [10, 20, 50, 100]\n",
    "powers = []\n",
    "for size in sample_sizes:\n",
    "    powers.append(get_power(0.5, size, 0.05))\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(sample_sizes, powers, marker='o', ls='dashed')\n",
    "plt.xlabel('sample size')\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Power increases as sample size increases, all other things being the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens to the power of a two-sided t-test as the effect size we want to detect changes? Create a plot to show how power changes as the effect size changes.** \n",
    "\n",
    "Use `effect_sizes = [0.1, 0.2, 0.5, 0.8]`. \n",
    "\n",
    "Assume alpha=0.05 and sample_size=100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "effect_sizes = [0.1, 0.2, 0.5, 0.8]\n",
    "powers = []\n",
    "for es in effect_sizes:\n",
    "    powers.append(get_power(es, 100, 0.05))\n",
    "\n",
    "plt.plot(effect_sizes, powers, marker='o', ls='dashed')\n",
    "plt.xlabel('effect size')\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Power increases as the effect size increases, all other things being the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens to the power of a statistical test as $\\alpha$ changes? Create a plot to show how power changes as $\\alpha$ changes.** \n",
    "\n",
    "Use `alphas = [0.001, 0.01, 0.05, 0.1, 0.2]`. \n",
    "\n",
    "Assume sample_size=100 and that you're trying to measure an effect_size = 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "powers = []\n",
    "for alpha in alphas:\n",
    "    powers.append(get_power(0.5, 100, alpha))\n",
    "\n",
    "plt.plot(alphas, powers, marker='o', ls='dashed')\n",
    "plt.xlabel('alphas')\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> As alpha increases, power increases, all other things remaining the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Note to instructors**: \n",
    "\n",
    "* Spend some time highlighting the deeper meaning of what they've just calculated: \n",
    "    * If you're willing to accept making more Type I error, you decrease the Type II error of the test (increase the power of the test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How is all of this useful?: Power analysis for experimental design\n",
    "\n",
    "You've seen that the power of a statistical test increases as sample size is increased, the effect size is increased, or the significance level $\\alpha$ is increased. \n",
    "\n",
    "When designing an experiment, before gathering samples, we should determine the sample size we need if we want our test to be powerful enough to detect differences between groups. \n",
    "* If we know the desired significance level, the desired power of our test, and the magnitude of the effect size we're trying to measure, we can determine the sample size we need to have to detect a difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study\n",
    "\n",
    "A researcher wants to compare two different diets, diets A and B, in diabetic mice. The researcher hypothesizes that diet A will be better than diet B, in terms of lower blood glucose. She gets a random sample of 30 diabetic mice, and randomly assigns them to one of the two diets. At the end of the experiment, which lasts 10 weeks, a blood glucose test will be conducted on each mouse. She expects the average difference in blood glucose measurement between the two groups of mice to be about 10 mg/dl. Based on past results, a common standard deviation of 15 mg/dl will be used for each treatment group in the power analysis. \n",
    "\n",
    "Is this a practical experimental design? Perform a power analysis simulation. Assume $\\alpha = 0.05$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 15 \n",
    "\n",
    "diet_A_mean = 0\n",
    "diet_A_sd = 15 \n",
    "\n",
    "diet_B_mean = 10\n",
    "diet_B_sd = diet_A_sd\n",
    "\n",
    "n_sim = 1000\n",
    "\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "n_rejections = 0\n",
    "\n",
    "for i in range(n_sim):\n",
    "    diet_A = stats.norm(loc=diet_A_mean, scale=diet_A_sd).rvs(sample_size)\n",
    "    diet_B = stats.norm(loc=diet_B_mean, scale=diet_B_sd).rvs(sample_size)\n",
    "    \n",
    "    result = stats.ttest_ind(diet_A, diet_B)\n",
    "    \n",
    "    if result[1] < alpha:\n",
    "        n_rejections+=1 \n",
    "\n",
    "power = n_rejections/n_sim\n",
    "\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The statistical power of this test, as designed, is 40.7%. If a difference of 5 mg/dl is actually present between diet A and diet B groups, the null hypothesis will be rejected 40.7% of the times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is the required sample size to identify a difference of 5 mg/dl between the groups with 80% power?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower, TTestPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "power = TTestIndPower()\n",
    "power.solve_power(effect_size = 5/10, alpha=0.05, power=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the smallest difference in blood glucose levels that the researcher can currently detect given her current sample size, if she wants the power of her test to be 80% and she wants to present results at $\\alpha = 0.05$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "effect_size = power.solve_power(alpha=0.05, power=0.8, nobs1=15)\n",
    "difference = effect_size*diet_A_sd\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* A Type I error is made when the null hypothesis is rejected given it's true. Type I error rate is denoted by $\\alpha$ and is equal to the significance level of our test.  \n",
    "* A Type II error is made when we fail to reject the null hypothesis given it's false. Type II error rate is denoted by $\\beta$. \n",
    "* Power is the probability of rejecting the null hypothesis given it's false. $\\text{power} = 1 - \\beta$. \n",
    "* The statistical power of a test is determined by $\\alpha$, sample size, and the effect size we're trying to measure. \n",
    "* Cohen's d is a standardized measure of the difference between two sample means. \n",
    "* We use statistical power analysis to determine the power of an already-designed experiment, or alternatively, we use statistical power analysis to determine the sample size we need to measure a given effect size in an experiment, should it exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another Case study: \n",
    "\n",
    "A teacher wants to study how daily tutoring sessions for students will affect students' overall test grades. \n",
    "\n",
    "The study will allow the enrollment of 30 students. Half will be randomized to a control group and not undergo any tutoring; the other half will receive daily tutoring sessions at the end of the class day. The tutoring will be carried out over 30 days. \n",
    "\n",
    "The teacher wants to know whether mean student grades after 30 days differ between the two groups in the study, those who were tutored and those who were not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What's the null hypothesis in this case? What's the alternative hypothesis?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Null hypothesis: There is no difference between experimental and control group mean grades.  \n",
    "\n",
    "> Alternative hypothesis: There is a difference between experimental and control group mean grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The teacher wants to know what power will be obtained under the sample size restrictions to identify a change in the mean grade of 5 points. Based on past results, a common standard deviation of 10 will be used for each treatment group in the power analysis. \n",
    "\n",
    "Perform a power analysis simulation to determine if this is a practical experimental design. Use $\\alpha = 0.05$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Significance level \n",
    "alpha = 0.05\n",
    "\n",
    "# Number of patients in each group\n",
    "sample_size = 15\n",
    "\n",
    "# Control group\n",
    "control_mean = 0\n",
    "control_sd = 10\n",
    "\n",
    "# Experimental group\n",
    "experimental_mean = 5\n",
    "experimental_sd = 10\n",
    "\n",
    "#Set the number of simulations for our test = 1000\n",
    "n_sim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# For reproducibility  \n",
    "np.random.seed(42)\n",
    "\n",
    "c = 0\n",
    "\n",
    "#  Run a for loop for range of values in n_sim\n",
    "\n",
    "for i in range(n_sim):\n",
    "\n",
    "    control = np.random.normal(loc= control_mean, scale=control_sd, size=sample_size)\n",
    "    experimental = np.random.normal(loc= experimental_mean, scale=experimental_sd, size=sample_size)\n",
    "    t_test = stats.ttest_ind(control, experimental)\n",
    "    if t_test[1] < alpha:\n",
    "        c+=1\n",
    "\n",
    "# number of null hypothesis rejections = c\n",
    "power = c/float(n_sim)\n",
    "\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The power of this statistical test is 0.244 or 24.4%. This result indicates that using 15 participants in each group and with given statistics, the statistical power of the experiment is 24.4%. \n",
    "\n",
    ">If a difference in mean test scores of 5 points is present between control and experimental groups, the null hypothesis would be rejected 24.4% of the times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What sample size would we need to detect a 5 point difference in test scores with a power of 80% and $\\alpha = 0.05$?**\n",
    "\n",
    "Hint: What effect size does this difference correspond to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# what's the effect size we are trying to detect?  \n",
    "# mean difference divided by the standard deviation\n",
    "d = 5/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "power = TTestIndPower()\n",
    "power.solve_power(effect_size=d, alpha=0.05, power=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> We would need samples of size 64 to detect a 5 point difference in test scores with a power of 80% and $\\alpha = 0.05$. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
